{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from keypoint_detection.utils.visualization import overlay_image_with_heatmap\n",
    "from keypoint_detection.models.detector import KeypointDetector\n",
    "from keypoint_detection.data.unlabeled_dataset import UnlabeledKeypointsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Get Model checkpoint from wandb\n",
    "\n",
    "\n",
    "checkpoint_reference = 'airo-box-manipulation/clothes/model-1kkjks2v:v1'\n",
    "\n",
    "# download checkpoint locally (if not already cached)\n",
    "run = wandb.init(project=\"clothes\", entity=\"airo-box-manipulation\")\n",
    "artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# \n",
    "#checkpoint = torch.load(Path(artifact_dir) / \"model.ckpt\")\n",
    "#print(checkpoint[\"hyper_parameters\"])\n",
    "# load checkpoint\n",
    "# ,map_location={\"cuda:0\":\"cpu\"}\n",
    "model = KeypointDetector.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\",backbone_type='ConvNeXtUnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "home = os.path.expanduser(\"~\")\n",
    "dataset_dir = os.path.join(home, \"cloth-keypoint-generation\", \"datasets\", \"towel_dataset_2\")\n",
    "JSON_PATH = os.path.join(dataset_dir, \"annotations.json\")\n",
    "IMAGE_DIR = os.path.join(dataset_dir, \"images\")\n",
    "\n",
    "\n",
    "print(JSON_PATH)\n",
    "dataset = UnlabeledKeypointsDataset(IMAGE_DIR)\n",
    "print(len(dataset))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size= 4, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    plot Tensor as image\n",
    "    images are kept in the [0,1] range, although in theory [-1,1] should be used to whiten..\n",
    "    \"\"\"\n",
    "    np_img = img.numpy()\n",
    "    # bring (C,W,H) to (W,H,C) dims\n",
    "    img = np.transpose(np_img, (1,2,0))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform  = torchvision.transforms.Resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop(img_batch, start_v, height, start_u, width):\n",
    "#     return img_batch[:,:,start_v: start_v +height, start_u: start_u + width]\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "# cropped  =crop(img, 250, 350, 300, 450)\n",
    "imshow(batch[0])\n",
    "# imshow(cropped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show_results(False)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = transform(batch)\n",
    "    channel = 0\n",
    "    output = model(batch)[:,channel]\n",
    "    print(output.shape)\n",
    "    print(batch[0].shape)\n",
    "    overlayed = overlay_image_with_heatmap(batch, output)\n",
    "    first = overlayed[0]\n",
    "    print()\n",
    "    first = first.permute((1,2,0))\n",
    "    plt.imshow(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('keypoint-detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ebc60e4e73209e16096be3b13acee9d1da114b499be2a6db93f392e5e51725a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
